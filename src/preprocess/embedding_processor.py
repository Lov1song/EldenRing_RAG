import torch
import json
import numpy as np
import os  # æ–°å¢ï¼šç”¨äºè·¯å¾„æ£€æŸ¥
from transformers import AutoTokenizer, AutoModel
from base_processor import BaseProcessor

def encode(model, tokenizer, texts, batch_size=32):
    all_embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        encoded_input = tokenizer(batch, padding=True, truncation=True, return_tensors='pt')
        with torch.no_grad():
            model_output = model(** encoded_input)
        embeddings = model_output.last_hidden_state.mean(dim=1)
        embeddings = torch.nn.functional.normalize(embeddings, p=2, dim=1)
        all_embeddings.append(embeddings)
    return torch.cat(all_embeddings)

class EmbeddingProcessor(BaseProcessor):
    def __init__(self):
        self.tokenizer = None
        self.model = None
    
    def _load_model(self, model_path: str):
        """å»¶è¿ŸåŠ è½½æ¨¡å‹ï¼Œæ·»åŠ è·¯å¾„æ ¡éªŒ"""
        if not os.path.exists(model_path):  # æ–°å¢ï¼šè·¯å¾„æ£€æŸ¥
            raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼š{model_path}ï¼Œè¯·æ£€æŸ¥é…ç½®")
        if not self.tokenizer or not self.model:
            try:  # æ–°å¢ï¼šæ•è·åŠ è½½å¼‚å¸¸
                self.tokenizer = AutoTokenizer.from_pretrained(model_path)
                self.model = AutoModel.from_pretrained(model_path)
                print(f"âœ… åŠ è½½æœ¬åœ°æ¨¡å‹æˆåŠŸï¼š{model_path}")
            except Exception as e:
                raise RuntimeError(f"æ¨¡å‹åŠ è½½å¤±è´¥ï¼š{str(e)}")
    
    def process(self, input_path: str, output_path: str, config: dict = None):
        default_config = {
            "model_path": "./models/all-MiniLM-L6-v2",
            "batch_size": 32
        }
        config = {** default_config, **(config or {})}
        
        self._load_model(config["model_path"])
        
        # æ–°å¢ï¼šæ£€æŸ¥è¾“å…¥æ–‡ä»¶å­˜åœ¨æ€§
        if not os.path.exists(input_path):
            raise FileNotFoundError(f"è¾“å…¥æ–‡ä»¶ä¸å­˜åœ¨ï¼š{input_path}")
        
        with open(input_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        texts = data["text"]
        if not texts:
            print("âš  æœªè¯»å–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼Œè·³è¿‡embeddingç”Ÿæˆ")
            return
        print(f"ğŸ“„ è¯»å–åˆ° {len(texts)} æ¡åˆ†å—æ–‡æœ¬")
        
        embeddings = encode(self.model, self.tokenizer, texts, batch_size=config["batch_size"])
        
        # æ–°å¢ï¼šç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(output_path), exist_ok=True)
        np.save(output_path, embeddings.cpu().numpy())
        print(f"âœ” Embeddingç”Ÿæˆå®Œæˆï¼å…± {len(texts)} æ¡å‘é‡ï¼Œä¿å­˜åˆ°ï¼š{output_path}")
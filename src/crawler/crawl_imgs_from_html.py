# src/crawl/crawl_imgs_from_html.py
import os
import re
import requests
import json
from bs4 import BeautifulSoup
from PIL import Image
import sys
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(__file__))))
import config

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆä¿®å¤æœ‰æ•ˆå›¾ç‰‡è¯¯åˆ¤ï¼‰=====================
HTML_ROOT_DIR = getattr(config, "HTML_ROOT_DIR", "./data/html_pages")
IMG_SAVE_ROOT = getattr(config, "IMG_ROOT_DIR", "./data/game_images")
HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36",
    "Referer": "https://www.gamersky.com/"  # å…³é”®ï¼šæ¨¡æ‹Ÿæ¸¸æ°‘æ˜Ÿç©ºrefererï¼Œé¿å…403
}
RETRY_TIMES = 3
# ä»…è¿‡æ»¤çœŸæ­£çš„å¹¿å‘ŠåŸŸåï¼ˆæ’é™¤img1.gamersky.comï¼‰
INVALID_IMG_DOMAINS = ["ad.gamersky.com", "banner.gamersky.com", "logo.gamersky.com"]
# å¹¿å‘Šå…³é”®è¯ï¼ˆå…¶ä»–æ¸¸æˆ/æ— å…³å†…å®¹ï¼‰
AD_KEYWORDS = ["SILKSONG", "åŸç¥", "ç‹è€…è£è€€", "å’Œå¹³ç²¾è‹±", "æ‰‹æ¸¸", "ç«¯æ¸¸", "æ–°æ¸¸"]
# è‰¾å°”ç™»æ³•ç¯ä¸“å±å…³é”®è¯ï¼ˆæ–‡å­—+URLè·¯å¾„ç‰¹å¾ï¼‰
ELDENRING_KEYWORDS = [
    # æ–‡å­—ç‰¹å¾
    "eldenring", "è‰¾å°”ç™»æ³•ç¯", "è‰¾å°”ç™»", "Elden Ring", "äº¤ç•Œåœ°", "é»„é‡‘æ ‘", "æ¢…ç³å¨œ",
    # URLè·¯å¾„ç‰¹å¾ï¼ˆä»ä½ çš„æœ‰æ•ˆURLæå–ï¼Œå…³é”®ä¿®å¤ï¼‰
    "image2022/02", "20220224_ax_156_1", "ax_156_1"
]
VALID_IMG_EXT = ["jpg", "jpeg", "png", "gif"]
MIN_IMG_SIZE = 1024  # 1KBï¼ˆè¿‡æ»¤æå°å¹¿å‘Šå›¾ï¼‰
MIN_IMG_DIMENSION = 300  # å®½/é«˜â‰¥300pxï¼ˆè¿‡æ»¤å°å¹¿å‘Šï¼‰

# ===================== å·¥å…·å‡½æ•° =====================
def complete_url(relative_url, base_url="https://www.gamersky.com"):
    """è¡¥å…¨URLï¼Œé€‚é…img1.gamersky.comçš„è·¯å¾„"""
    if not relative_url:
        return ""
    if relative_url.startswith("http"):
        return relative_url
    elif relative_url.startswith("//"):
        return "https:" + relative_url
    elif relative_url.startswith("/"):
        return base_url + relative_url
    else:
        return base_url + "/" + relative_url

def is_valid_img(img_url):
    """ä¿®å¤è¯¯åˆ¤ï¼šæ£€æŸ¥å¹¿å‘Š+è‰¾å°”ç™»ç‰¹å¾ï¼ˆæ–‡å­—/è·¯å¾„ï¼‰"""
    if not img_url:
        return False
    # 1. è¿‡æ»¤å¹¿å‘ŠåŸŸå
    if any(domain in img_url for domain in INVALID_IMG_DOMAINS):
        return False
    # 2. è¿‡æ»¤å…¶ä»–æ¸¸æˆå¹¿å‘Š
    if any(kw in img_url for kw in AD_KEYWORDS):
        return False
    # 3. åŒ¹é…è‰¾å°”ç™»æ³•ç¯çš„æ–‡å­—æˆ–URLè·¯å¾„ç‰¹å¾
    for kw in ELDENRING_KEYWORDS:
        if kw in img_url:
            return True
    return False

def is_valid_img_dimension(img_path):
    """è¿‡æ»¤å°å°ºå¯¸å¹¿å‘Šå›¾"""
    try:
        with Image.open(img_path) as img:
            width, height = img.size
            return width >= MIN_IMG_DIMENSION or height >= MIN_IMG_DIMENSION
    except Exception as e:
        print(f"âš ï¸ æ£€æŸ¥å°ºå¯¸å¤±è´¥ï¼ˆ{img_path}ï¼‰ï¼š{e}")
        return False

# ===================== å•å¼ HTMLå›¾ç‰‡æå– =====================
def crawl_imgs_from_single_html(html_path, img_save_dir, page_num):
    # è¯»å–HTML
    try:
        with open(html_path, "r", encoding="utf-8") as f:
            html_content = f.read()
    except Exception as e:
        print(f"âŒ è¯»å–HTMLå¤±è´¥ï¼š{e}")
        return []

    # è§£æimgæ ‡ç­¾ï¼ˆä¼˜å…ˆdata-srcï¼Œé€‚é…æ‡’åŠ è½½ï¼‰
    soup = BeautifulSoup(html_content, "html.parser")
    img_tags = soup.find_all("img")
    print(f"ğŸ” æå–åˆ°{len(img_tags)}ä¸ªimgæ ‡ç­¾")

    img_urls = []
    for idx, img_tag in enumerate(img_tags):
        # ä¼˜å…ˆå–æ‡’åŠ è½½å±æ€§ï¼Œå†å–src
        img_url = img_tag.get("data-src") or img_tag.get("src") or img_tag.get("data-original")
        if img_url:
            full_url = complete_url(img_url)
            if is_valid_img(full_url):
                img_urls.append(full_url)
                print(f"   âœ… æœ‰æ•ˆURL {idx+1}ï¼š{full_url}ï¼ˆå«è‰¾å°”ç™»ç‰¹å¾ï¼‰")
            else:
                print(f"   âŒ è¿‡æ»¤URL {idx+1}ï¼š{full_url}ï¼ˆæ— è‰¾å°”ç™»ç‰¹å¾/å¹¿å‘Šï¼‰")
        else:
            print(f"   âŒ æ— æ•ˆæ ‡ç­¾ {idx+1}ï¼šæ— src/data-srcå±æ€§")

    if not img_urls:
        print(f"âš ï¸ æ— æœ‰æ•ˆè‰¾å°”ç™»æ³•ç¯å›¾ç‰‡")
        return []

    # ä¸‹è½½å›¾ç‰‡ï¼ˆå¸¦é‡è¯•+å°ºå¯¸è¿‡æ»¤ï¼‰
    downloaded_imgs = []
    for img_idx, img_url in enumerate(img_urls, 1):
        # ç”Ÿæˆæ–‡ä»¶åï¼ˆpage_é¡µç _åºå·.åç¼€ï¼‰
        img_ext = img_url.split(".")[-1].lower() if "." in img_url else "jpg"
        if img_ext not in VALID_IMG_EXT:
            img_ext = "jpg"
        img_filename = f"page_{page_num}_{img_idx}.{img_ext}"
        img_save_path = os.path.join(img_save_dir, img_filename)

        # é¿å…é‡å¤ä¸‹è½½
        if os.path.exists(img_save_path):
            if is_valid_img_dimension(img_save_path):
                print(f"â„¹ï¸ å·²å­˜åœ¨æœ‰æ•ˆå›¾ç‰‡ï¼Œè·³è¿‡ï¼š{img_filename}")
                downloaded_imgs.append({"filename": img_filename, "path": img_save_path})
            else:
                print(f"â„¹ï¸ å·²å­˜åœ¨ä½†å°ºå¯¸è¿‡å°ï¼Œåˆ é™¤ï¼š{img_filename}")
                os.remove(img_save_path)
            continue

        # ä¸‹è½½é‡è¯•é€»è¾‘
        success = False
        for retry in range(RETRY_TIMES):
            try:
                response = requests.get(img_url, headers=HEADERS, timeout=15, stream=True)
                if response.status_code == 200:
                    img_size = len(response.content)
                    # è¿‡æ»¤æå°æ–‡ä»¶
                    if img_size < MIN_IMG_SIZE:
                        print(f"âš ï¸ æ–‡ä»¶è¿‡å°ï¼ˆ{img_size}Bï¼‰ï¼Œè·³è¿‡ï¼š{img_url}")
                        break
                    # ä¿å­˜å›¾ç‰‡
                    with open(img_save_path, "wb") as f:
                        f.write(response.content)
                    # äºŒæ¬¡è¿‡æ»¤å°ºå¯¸
                    if is_valid_img_dimension(img_save_path):
                        print(f"âœ… ä¸‹è½½æˆåŠŸï¼š{img_filename}ï¼ˆ{img_size/1024:.2f}KBï¼Œå°ºå¯¸åˆè§„ï¼‰")
                        downloaded_imgs.append({"filename": img_filename, "path": img_save_path})
                    else:
                        print(f"âš ï¸ å°ºå¯¸è¿‡å°ï¼ˆ<{MIN_IMG_DIMENSION}pxï¼‰ï¼Œåˆ é™¤ï¼š{img_filename}")
                        os.remove(img_save_path)
                    success = True
                    break
                else:
                    print(f"âš ï¸ çŠ¶æ€ç {response.status_code}ï¼Œé‡è¯•{retry+1}/{RETRY_TIMES}")
            except Exception as e:
                print(f"âš ï¸ ä¸‹è½½å¼‚å¸¸ï¼ˆ{e}ï¼‰ï¼Œé‡è¯•{retry+1}/{RETRY_TIMES}")

        if not success:
            print(f"âŒ ä¸‹è½½å¤±è´¥ï¼ˆè¶…è¿‡é‡è¯•æ¬¡æ•°ï¼‰ï¼š{img_url}")

    return downloaded_imgs

# ===================== æ‰¹é‡å¤„ç† =====================
def batch_crawl_imgs_by_timestamp(target_timestamp=None):
    # ç¡®è®¤HTMLç›®å½•
    if not os.path.exists(HTML_ROOT_DIR):
        print(f"âŒ HTMLæ ¹ç›®å½•ä¸å­˜åœ¨ï¼š{HTML_ROOT_DIR}")
        return

    # é€‰æ‹©ç›®æ ‡æ‰¹æ¬¡ï¼ˆæŒ‡å®š/æœ€æ–°ï¼‰
    all_timestamps = [d for d in os.listdir(HTML_ROOT_DIR) if os.path.isdir(os.path.join(HTML_ROOT_DIR, d))]
    if not all_timestamps:
        print(f"âŒ æ— æ—¶é—´æˆ³æ‰¹æ¬¡ç›®å½•")
        return

    if target_timestamp and target_timestamp in all_timestamps:
        html_batch_dir = os.path.join(HTML_ROOT_DIR, target_timestamp)
    else:
        all_timestamps.sort(reverse=True)
        target_timestamp = all_timestamps[0]
        html_batch_dir = os.path.join(HTML_ROOT_DIR, target_timestamp)
    print(f"ğŸ“Œ å¼€å§‹å¤„ç†æ‰¹æ¬¡ï¼š{target_timestamp}")
    print(f"ğŸ” HTMLç›®å½•ï¼š{html_batch_dir}")

    # åˆ›å»ºå›¾ç‰‡ä¿å­˜ç›®å½•
    img_batch_dir = os.path.join(IMG_SAVE_ROOT, target_timestamp)
    os.makedirs(img_batch_dir, exist_ok=True)
    print(f"ğŸ“ å›¾ç‰‡ä¿å­˜ç›®å½•ï¼š{img_batch_dir}")

    # éå†HTMLæ–‡ä»¶ï¼ˆæŒ‰é¡µç æ’åºï¼‰
    html_files = [f for f in os.listdir(html_batch_dir) if f.startswith("page_") and f.endswith(".html")]
    if not html_files:
        print(f"âŒ æ— page_*.htmlæ–‡ä»¶")
        return
    html_files.sort(key=lambda x: int(re.findall(r"page_(\d+)\.html", x)[0]))

    # æµ‹è¯•æ¨¡å¼ï¼šå…ˆå¤„ç†å‰3ä¸ªHTMLï¼ˆå¿«é€ŸéªŒè¯ï¼‰
    test_html_files = html_files
    print(f"\nâš ï¸ æµ‹è¯•æ¨¡å¼ï¼šå¤„ç†å‰{len(test_html_files)}ä¸ªHTMLï¼ˆéªŒè¯æœ‰æ•ˆå›¾ç‰‡ï¼‰")

    total_downloaded = 0
    for html_file in test_html_files:
        page_num = re.findall(r"page_(\d+)\.html", html_file)[0]
        html_path = os.path.join(html_batch_dir, html_file)
        print(f"\n===== å¤„ç†ç¬¬{page_num}é¡µï¼š{html_file} =====")
        
        # æå–å¹¶ä¸‹è½½å›¾ç‰‡
        page_imgs = crawl_imgs_from_single_html(html_path, img_batch_dir, page_num)
        total_downloaded += len(page_imgs)

    # æœ€ç»ˆç»Ÿè®¡
    print(f"\nğŸ‰ æ‰¹æ¬¡å¤„ç†å®Œæˆï¼")
    print(f"ğŸ“Š ç»Ÿè®¡ï¼šå¤„ç†{len(test_html_files)}ä¸ªHTMLï¼Œä¸‹è½½{total_downloaded}å¼ æœ‰æ•ˆæ¸¸æˆå›¾")
    print(f"ğŸ—‚ï¸  å›¾ç‰‡ç›®å½•ï¼š{img_batch_dir}")
    print(f"\nğŸ’¡ æç¤ºï¼šè‹¥éœ€å¤„ç†å…¨éƒ¨HTMLï¼Œåˆ é™¤'test_html_files = html_files[:3]'å³å¯")

if __name__ == "__main__":
    # æ›¿æ¢ä¸ºä½ çš„å®é™…æ‰¹æ¬¡æ—¶é—´æˆ³ï¼ˆå¦‚"20251203_151414"ï¼‰
    batch_crawl_imgs_by_timestamp(target_timestamp="20251203_155633")
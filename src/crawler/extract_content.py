import os
from bs4 import BeautifulSoup
import re
from datetime import datetime  # æ–°å¢ï¼šç”¨äºç”Ÿæˆæ—¶é—´æˆ³

# é…ç½®
HTML_DIR = "./data/html_pages"  # å­˜æ”¾çˆ¬å–çš„HTMLæ–‡ä»¶å¤¹ï¼ˆå¯æ”¹ä¸ºå…·ä½“æ—¶é—´æˆ³æ–‡ä»¶å¤¹è·¯å¾„ï¼Œå¦‚ ./data/html_pages/20251203_153020ï¼‰
OUTPUT_ROOT_DIR = "./data/extracted_text"  # æå–æ–‡æœ¬çš„æ ¹ç›®å½•
TIMESTAMP_FORMAT = "%Y%m%d_%H%M%S"  # æ—¶é—´æˆ³æ ¼å¼ï¼ˆå’Œçˆ¬è™«è„šæœ¬ä¸€è‡´ï¼‰

def clean_text(text):
    """æ¸…æ´—æ–‡æœ¬ï¼šå»é™¤å¤šä½™ç©ºç™½ã€ç‰¹æ®Šå­—ç¬¦"""
    # å»é™¤è¿ç»­ç©ºç™½ï¼ˆæ¢è¡Œã€ç©ºæ ¼ç­‰ï¼‰
    text = re.sub(r'\s+', ' ', text).strip()
    # å»é™¤ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡ã€è‹±æ–‡ã€æ•°å­—å’ŒåŸºæœ¬æ ‡ç‚¹ï¼‰
    text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9,.!?;:\'\"()ï¼ˆï¼‰ã€Šã€‹<>]', ' ', text)
    return text

def extract_main_content(html_path):
    """ä»å•é¡µHTMLä¸­æå–æ­£æ–‡"""
    with open(html_path, "r", encoding="utf-8") as f:
        html = f.read()
    
    soup = BeautifulSoup(html, "html.parser")
    # å®šä½æ­£æ–‡å®¹å™¨ï¼ˆæ ¸å¿ƒæ ‡ç­¾ï¼šclass="Mid2L_con"ï¼‰
    content_container = soup.find("div", class_="Mid2L_con")
    if not content_container:
        return ""
    
    # ç§»é™¤æ— å…³å…ƒç´ ï¼ˆåˆ†é¡µæ§ä»¶ã€å¹¿å‘Šã€è„šæœ¬ç­‰ï¼‰
    for tag in content_container.find_all([
        "div", "script", "style", "iframe",  # å¯èƒ½åŒ…å«å¹¿å‘Š/æ§ä»¶çš„æ ‡ç­¾
        "a"  # è¶…é“¾æ¥ï¼ˆéæ­£æ–‡å†…å®¹ï¼‰
    ]):
        # ä¿ç•™<p>æ ‡ç­¾å†…çš„æ–‡æœ¬ï¼Œä½†ç§»é™¤<a>é“¾æ¥æœ¬èº«
        if tag.name == "a":
            tag.extract()
        # ç§»é™¤åˆ†é¡µæ§ä»¶ï¼ˆ<div class="post_ding_top">ï¼‰
        elif tag.get("class") and "post_ding_top" in tag.get("class"):
            tag.extract()
        # ç§»é™¤å¹¿å‘Šç›¸å…³divï¼ˆå¦‚åŒ…å«"advert"ç­‰å…³é”®è¯ï¼‰
        elif "advert" in tag.get("class", []) or "ad" in tag.get("id", ""):
            tag.extract()
    
    # æå–æ‰€æœ‰æ–‡æœ¬å¹¶æ¸…æ´—
    raw_text = content_container.get_text()
    cleaned_text = clean_text(raw_text)
    return cleaned_text

def batch_extract(target_html_dir=None, custom_timestamp=None):
    """
    æ‰¹é‡å¤„ç†æ‰€æœ‰HTMLæ–‡ä»¶
    :param target_html_dir: å¯é€‰ï¼ŒæŒ‡å®šè¦å¤„ç†çš„HTMLæ–‡ä»¶å¤¹ï¼ˆå¦‚ ./data/html_pages/20251203_153020ï¼‰
    :param custom_timestamp: å¯é€‰ï¼Œè‡ªå®šä¹‰æ—¶é—´æˆ³ï¼ˆç”¨äºå…³è”çˆ¬å–æ‰¹æ¬¡ï¼‰
    """
    # 1. ç¡®å®šè¦å¤„ç†çš„HTMLæ–‡ä»¶å¤¹ï¼ˆé»˜è®¤ç”¨é…ç½®ä¸­çš„HTML_DIRï¼‰
    current_html_dir = target_html_dir or HTML_DIR
    if not os.path.exists(current_html_dir):
        print(f"é”™è¯¯ï¼šHTMLæ–‡ä»¶å¤¹ä¸å­˜åœ¨ â†’ {current_html_dir}")
        return
    
    # 2. ç”Ÿæˆæ—¶é—´æˆ³ï¼ˆé»˜è®¤ç”¨å½“å‰æ—¶é—´ï¼Œæ”¯æŒè‡ªå®šä¹‰å…³è”çˆ¬å–æ‰¹æ¬¡ï¼‰
    timestamp = custom_timestamp or datetime.now().strftime(TIMESTAMP_FORMAT)
    
    # 3. åˆ›å»ºå¸¦æ—¶é—´æˆ³çš„è¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆæ ¹ç›®å½•+æ—¶é—´æˆ³å­ç›®å½•ï¼‰
    output_timestamp_dir = os.path.join(OUTPUT_ROOT_DIR, timestamp)
    if not os.path.exists(output_timestamp_dir):
        os.makedirs(output_timestamp_dir)
    
    # 4. éå†æ‰€æœ‰HTMLæ–‡ä»¶
    html_files = [f for f in os.listdir(current_html_dir) if f.endswith(".html")]
    if not html_files:
        print(f"è­¦å‘Šï¼š{current_html_dir} ä¸­æ²¡æœ‰æ‰¾åˆ°HTMLæ–‡ä»¶")
        return
    
    for filename in html_files:
        html_path = os.path.join(current_html_dir, filename)
        page_num = filename.split("_")[-1].split(".")[0]  # ä»page_1.htmlæå–é¡µç 
        
        # æå–æ­£æ–‡
        content = extract_main_content(html_path)
        if not content:
            print(f"è­¦å‘Šï¼š{filename} æœªæå–åˆ°æ­£æ–‡")
            continue
        
        # 5. ä¿å­˜åˆ°å¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å¤¹ä¸­
        output_path = os.path.join(output_timestamp_dir, f"content_{page_num}.txt")
        with open(output_path, "w", encoding="utf-8") as f:
            f.write(content)
        print(f"å·²æå– â†’ ç¬¬{page_num}é¡µ â†’ {output_path}")
    
    # 6. é¢å¤–ï¼šç”Ÿæˆåˆå¹¶åçš„æ€»æ–‡æœ¬æ–‡ä»¶ï¼ˆæ–¹ä¾¿åç»­RAGæµæ°´çº¿ç›´æ¥ä½¿ç”¨ï¼‰
    merged_content = ""
    for filename in sorted(html_files, key=lambda x: int(x.split("_")[-1].split(".")[0])):
        html_path = os.path.join(current_html_dir, filename)
        content = extract_main_content(html_path)
        if content:
            merged_content += content + "\n\n"  # æ¯é¡µæ–‡æœ¬ç”¨ä¸¤ä¸ªæ¢è¡Œåˆ†éš”
    
    merged_output_path = os.path.join(output_timestamp_dir, "merged_all.txt")
    with open(merged_output_path, "w", encoding="utf-8") as f:
        f.write(merged_content.strip())
    print(f"\nâœ… æ‰€æœ‰é¡µé¢æå–å®Œæˆï¼åˆå¹¶æ–‡ä»¶ä¿å­˜åˆ° â†’ {merged_output_path}")
    print(f"ğŸ“ æå–çš„æ–‡æœ¬æ€»ç›®å½• â†’ {output_timestamp_dir}")

if __name__ == "__main__":
    # ä¾‹å¦‚ï¼šå¤„ç†20251203_153020æ‰¹æ¬¡çš„çˆ¬å–ç»“æœï¼Œæå–åçš„æ–‡æœ¬ä¹Ÿç”¨åŒä¸€ä¸ªæ—¶é—´æˆ³
    target_html_dir = "./data/html_pages/20251203_155633"  # æ›¿æ¢ä¸ºä½ çš„çˆ¬å–æ‰¹æ¬¡æ–‡ä»¶å¤¹è·¯å¾„
    custom_timestamp = "20251203_155633"  # å’Œçˆ¬å–æ‰¹æ¬¡çš„æ—¶é—´æˆ³ä¸€è‡´
    batch_extract(target_html_dir=target_html_dir, custom_timestamp=custom_timestamp)